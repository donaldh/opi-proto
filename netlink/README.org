* Using netlink to mirror state to DPU hardware

All Linux user space applications use netlink messages, either directly or indirectly, to
program the kernel networking stack. Netlink provides fine-grained control over all aspects of
kernel networking and represents the scope of features required by OPI.

** Classic hardware offload

In the classical hardware offload approach, the driver must implement all offloads via the
~ndo_setup_tc~ hook in ~struct net_device_ops~, defined in [[https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h#L1493][include/linux/netdevice.h]].

#+begin_src dot :file "classic_offload.png" :exports results
 digraph classic_offload {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        app [shape=box]
        subgraph cluster_kernel {
                label = "kernel";
                tc [shape=box]
                driver [shape=box]
        }

        app -> tc [label="netlink"]
        tc -> driver [label="ops->ndo_setup_tc(...)"]
        driver -> nic
 }
#+end_src

#+RESULTS:
[[file:classic_offload.png]]

** Mirroring kernel networking state

DPU hardware could offload more than the existing capabilities that are supported by the
~ndo_setup_tc~ hook. For example routing tables could be mirrored to the DPU to allow for
hardware-accelerated route based forwarding. This could be done by using a netlink listener
consuming netlink notifications and using an out-of-band mechanism to program the equivalent
state on the DPU hardware.

#+begin_src dot :file mirror_state.png :exports results
 digraph mirror_state {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        subgraph user_space {
                app [shape=box]
                listener [shape=box]
                rank = same;
        }
        subgraph cluster_kernel {
                label = "kernel"
                labeljust = "l"
                tc [shape=box; label="tc / rtnl"]
                driver [shape=box]
        }

        app -> tc [label="netlink"]
        tc -> listener [label="notify"]
        tc -> driver [label="ops->ndo_setup_tc(...)"]
        driver -> nic

        listener -> nic [label="oob programming"]

        edge[style=invis]
        listener->tc->driver->nic
 }
#+end_src

#+RESULTS:
[[file:mirror_state.png]]

** Extending offloads with BPF

It might be possible to use BPF ~struct_ops~ to provide a way to extend offload capabilities
without driver development work.

#+begin_src dot :file bpf_enablement.png :tangle bpf_enablement.txt :exports results
 digraph mirror_state {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        subgraph user_space {
                app [shape=box]
                listener [shape=box label="User space helper"]
                rank = same;
        }
        subgraph cluster_kernel {
                label = "kernel"
                labeljust = "l"
                tc [shape=box; label="tc / rtnl"]
                driver [shape=box]
                bpf [shape=box label="BPF prog"]
                map [shape=box]
                {rank = same; driver; bpf; map;}
        }

        app -> tc [label="netlink"]
        bpf -> listener [label="ringbuf"]
        tc -> driver [label="ops->ndo_setup_tc(...)"]
        driver -> nic
        driver -> bpf
        bpf -> map

        listener -> nic [label="oob programming"]

        edge[style=invis]
        listener->tc->bpf->nic
        driver->bpf->map
        app->listener

 }
#+end_src

#+RESULTS:
[[file:bpf_enablement.png]]

** Switchdev offloads with BPF

This diagram shows roughly what Anton has prototyped for switchdev notifier offloads.

Each of the switchdev notifier types could be handled by:
1. A driver dispatching them to a registered BPF program
2. The BPF program sending them on a ringbuf to userspace

#+begin_src C
enum switchdev_notifier_type {
	SWITCHDEV_FDB_ADD_TO_BRIDGE = 1,
	SWITCHDEV_FDB_DEL_TO_BRIDGE,
	SWITCHDEV_FDB_ADD_TO_DEVICE,
	SWITCHDEV_FDB_DEL_TO_DEVICE,
	SWITCHDEV_FDB_OFFLOADED,
	SWITCHDEV_FDB_FLUSH_TO_BRIDGE,

	SWITCHDEV_PORT_OBJ_ADD, /* Blocking. */
	SWITCHDEV_PORT_OBJ_DEL, /* Blocking. */
	SWITCHDEV_PORT_ATTR_SET, /* May be blocking . */

	SWITCHDEV_VXLAN_FDB_ADD_TO_BRIDGE,
	SWITCHDEV_VXLAN_FDB_DEL_TO_BRIDGE,
	SWITCHDEV_VXLAN_FDB_ADD_TO_DEVICE,
	SWITCHDEV_VXLAN_FDB_DEL_TO_DEVICE,
	SWITCHDEV_VXLAN_FDB_OFFLOADED,

	SWITCHDEV_BRPORT_OFFLOADED,
	SWITCHDEV_BRPORT_UNOFFLOADED,
};
#+end_src


#+begin_src dot :file switchdev_offload.png :tangle switchdev_offload.txt :exports results
 digraph switchdev {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        subgraph user_space {
                app [shape=box]
                listener [shape=box label="User space helper\nw/ vendor libraries"]
                rank = same;
        }
        subgraph cluster_kernel {
                label = "kernel"
                labeljust = "l"
                bridge [shape=box; label="bridge"]
                ringbuf
                {rank = same; bridge; ringbuf}
                driver [shape=box label="driver\nw/ struct_ops BPF progs"]
                bpf [shape=box label="BPF prog"]
                map [shape=box]
                {rank = same; driver; bpf; map;}
        }

        app -> bridge [label="netlink"]
        bpf -> ringbuf -> listener
        bridge -> driver [label="nb->notifier_call(...)"]
        driver -> nic [label="traditional\nprogramming path"]
        driver -> bpf
        bpf -> map

        listener -> nic [label="out of band\nprogramming" style=dashed]

        edge[style=invis]
        listener->bridge->bpf->nic
        driver->bpf->map
        app->listener

 }
#+end_src

#+RESULTS:
[[file:switchdev_offload.png]]

** FIB offloads

The same mechanism used for switchdev FDB offloads could also be used for routing FIB offloads.

#+begin_src C
enum fib_event_type {
	FIB_EVENT_ENTRY_REPLACE,
	FIB_EVENT_ENTRY_APPEND,
	FIB_EVENT_ENTRY_ADD,
	FIB_EVENT_ENTRY_DEL,
	FIB_EVENT_RULE_ADD,
	FIB_EVENT_RULE_DEL,
	FIB_EVENT_NH_ADD,
	FIB_EVENT_NH_DEL,
	FIB_EVENT_VIF_ADD,
	FIB_EVENT_VIF_DEL,
};
#+end_src
