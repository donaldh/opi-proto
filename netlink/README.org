* Using netlink to mirror state to DPU hardware

All Linux user space applications use netlink messages, either directly or indirectly, to
program the kernel networking stack. Netlink provides fine-grained control over all aspects of
kernel networking and represents the scope of features required by OPI.

** Classic hardware offload

In the classical hardware offload approach, the driver must implement all offloads via the
~ndo_setup_tc~ hook in ~struct net_device_ops~, defined in [[https://elixir.bootlin.com/linux/latest/source/include/linux/netdevice.h#L1493][include/linux/netdevice.h]].

#+begin_src dot :file "classic_offload.png" :exports results
 digraph classic_offload {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        app [shape=box]
        subgraph cluster_kernel {
                label = "kernel";
                tc [shape=box]
                driver [shape=box]
        }

        app -> tc [label="netlink"]
        tc -> driver [label="ops->ndo_setup_tc(...)"]
        driver -> nic
 }
#+end_src

#+RESULTS:
[[file:classic_offload.png]]

** Mirroring kernel networking state

DPU hardware could offload more than the existing capabilities that are supported by the
~ndo_setup_tc~ hook. For example routing tables could be mirrored to the DPU to allow for
hardware-accelerated route based forwarding. This could be done by using a netlink listener
consuming netlink notifications and using an out-of-band mechanism to program the equivalent
state on the DPU hardware.

#+begin_src dot :file mirror_state.png :exports results
 digraph mirror_state {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        subgraph user_space {
                app [shape=box]
                listener [shape=box]
                rank = same;
        }
        subgraph cluster_kernel {
                label = "kernel"
                labeljust = "l"
                tc [shape=box; label="tc / rtnl"]
                driver [shape=box]
        }

        app -> tc [label="netlink"]
        tc -> listener [label="notify"]
        tc -> driver [label="ops->ndo_setup_tc(...)"]
        driver -> nic

        listener -> nic [label="oob programming"]

        edge[style=invis]
        listener->tc->driver->nic
 }
#+end_src

#+RESULTS:
[[file:mirror_state.png]]

** Extending offloads with BPF

It might be possible to use BPF ~struct_ops~ to provide a way to extend offload capabilities
without driver development work.

#+begin_src dot :file bpf_enablement.png :exports results
 digraph mirror_state {
        fontname="Arial"
        node [fontname="Arial"]
        edge [fontname="Arial"]

        subgraph user_space {
                app [shape=box]
                listener [shape=box label="User space helper"]
                rank = same;
        }
        subgraph cluster_kernel {
                label = "kernel"
                labeljust = "l"
                tc [shape=box; label="tc / rtnl"]
                driver [shape=box]
                bpf [shape=box label="BPF prog"]
                map [shape=box]
                {rank = same; driver; bpf; map;}
        }

        app -> tc [label="netlink"]
        bpf -> listener [label="ringbuf"]
        tc -> driver [label="ops->ndo_setup_tc(...)"]
        driver -> nic
        driver -> bpf
        bpf -> map

        listener -> nic [label="oob programming"]

        edge[style=invis]
        listener->tc->bpf->nic
        driver->bpf->map
        app->listener

 }
#+end_src

#+RESULTS:
[[file:bpf_enablement.png]]
